{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMC1cbc6DfzOmSJotd9RXoa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x-jp627jotqe"},"outputs":[],"source":["! pip install --upgrade codetiming numpy pandas census us mechanicalsoup geopandas pandas_bokeh torch\n","get_ipython().kernel.do_shutdown(True)"]},{"cell_type":"code","source":["mount_path = '/content/drive'\n","repo_path = f'{mount_path}/MyDrive/gerrymandering/2022-10/voting_predictor'\n","from google.colab import drive\n","drive.mount(mount_path)\n","%cd {repo_path}\n","%load_ext google.colab.data_table\n","%load_ext autoreload\n","%autoreload\n","from model import *\n","%cd {repo_path}\n","%load_ext google.colab.data_table\n"],"metadata":{"id":"up3Uik6vo9k9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload\n","from model import *\n","\n","a = ['s1501_c01_003ea']\n","get(a, 'acs5st', 2018, 'tract')\n","# census_session.acs5.state(('NAME', 'S1501_C01_053E'), '01')"],"metadata":{"id":"BQJd_o8KyoQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["5284-85-5144, 3467-85-2688"],"metadata":{"id":"D83DCnM1vbkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload\n","from model import *\n","\n","class Feedforward(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, activation):\n","        super(Feedforward, self).__init__()\n","        assert len(hidden_size) == len(activation), 'hidden_size and activation must have same length'\n","        L = []\n","        p = input_size\n","        for q, f in zip(hidden_size, activation):\n","            L.append(torch.nn.Linear(p, q))\n","            L.append(f())\n","            p = q\n","        self.nn = torch.nn.Sequential(*L)\n","        self.rmse_train = []\n","        self.rmse_test  = []\n","\n","    def forward(self, X, W):\n","        self.prop = self.nn(X)\n","        self.pred = ((self.prop * W) @ B).squeeze()\n","        return self.pred\n","\n","\n","def features(year, election='general', office='President', level='tract', overwrite=False):\n","    get_acs5(year=year, overwrite=overwrite)\n","    pq = data_path / f'features/{year}_{election}_{office}.parquet'\n","    if pq.is_file() and not overwrite:\n","        df = pd.read_parquet(pq)\n","    else:\n","        qry = f\"\"\"\n","        select\n","            vtd2020,\n","            election,\n","            office,\n","            lower(party) as party,\n","            campaign,\n","            -- party,\n","            votes,\n","        from\n","            elections.all\n","        where\n","            election = '{election}'\n","            and office = '{office}'\n","            and year = {year}\n","            and party in ('R', 'D')\n","        \"\"\"\n","        elections = query_to_df(qry).pivot(index=['vtd2020', 'campaign'], columns='party', values='votes').fillna(0)#.reset_index('campaign')#.set_index('vtd2020')\n","\n","        qry = f\"\"\"\n","        select\n","            A.vtd2020,\n","            A.county,\n","            cast(round(A.aland)             as int) as aland,\n","            cast(round(A.awater)            as int) as awater,\n","            cast(round(A.atot)              as int) as atot,\n","            cast(round(A.perim)             as int) as perim,\n","            cast(round(A.polsby_popper*100) as int) as polsby_popper,\n","            cast(round(A.dist_to_border)    as int) as dist_to_border,\n","        from\n","            shapes.vtd2020 as A\n","        \"\"\"\n","        shapes = query_to_df(qry).set_index('vtd2020')\n","\n","        qry = f\"\"\"select\n","            *\n","        from\n","            acs5.{year}\n","        \"\"\"\n","        acs = query_to_df(qry).set_index('vtd2020')\n","\n","        vtd = prep(elections.join(shapes, how='outer').join(acs, how='outer'))\n","        df = vtd\n","\n","        for race in ['all', 'white', 'hisp', 'other']:\n","            df[race+'_vap_density'] = (df[race+'_vap_pop'] / np.fmax(df['aland'], 1) * 1609.34**2)\n","        df['votes'] = df[elections.columns].sum(axis=1)\n","        for party in elections.columns:\n","            df[party+'_pct'] = df[party] / np.fmax(df['all_tot_pop'], 1) * 100\n","\n","        # for col in df.columns:\n","        #     a = col.find('_')+1\n","        #     b = col[a:].find('_')+a+1\n","        #     subpop = col[:b] + 'pop'\n","        #     if col[b:] != 'pop' and subpop in df.columns:\n","        #         df[col] /= np.fmax(df[subpop], 1)\n","        df_to_parquet(df, pq)\n","    \n","    return pq\n","\n","\n","elections = ['2020_general_President', '2016_general_President', '2018_general_USSen', '2020_general_USSen']\n","feat = [\n","    'polsby_popper', 'dist_to_border',\n","    'hisp_vap_spanish_at_home', 'hisp_vap_spanish_at_home_english_well',\n","    'all_tot_pop'       , 'white_tot_pop'       , 'hisp_tot_pop'       , 'other_tot_pop',\n","    'all_vap_pop'       , 'white_vap_pop'       , 'hisp_vap_pop'       , 'other_vap_pop',\n","    'all_vap_density'   , 'white_vap_density'   , 'hisp_vap_density'   , 'other_vap_density',\n","    'all_vap_poverty'   , 'white_vap_poverty'   , 'hisp_vap_poverty'   , 'other_vap_poverty',\n","    'all_vap_elderly'   , 'white_vap_elderly'   , 'hisp_vap_elderly'   , 'other_vap_elderly',\n","    'all_vap_highschool', 'white_vap_highschool', 'hisp_vap_highschool', 'other_vap_highschool',\n","]\n","targ = ['d', 'r']\n","\n","pqs = {elec: features(*elec.split('_')) for elec in elections}\n","for elec, pq in pqs.items():\n","    print(elec)\n","\n","    dfs = {elec: parquet_to_df(pq)[feat+targ].sample(frac=1).astype(float) for elec, pq in pqs.items()}\n","    display(dfs[elec].head(2))\n","    \n","    break\n","    # test = dfs.pop(elec)\n","    # train = pd.concat(dfs.values())\n","    # X_test  = torch.FloatTensor(test [targ].values).to(device)\n","    # Y_test  = torch.FloatTensor(test [feat].values).to(device)\n","    # X_train = torch.FloatTensor(train[targ].values).to(device)\n","    # Y_train = torch.FloatTensor(train[feat].values).to(device)\n","\n","    # model = Feedforward(\n","    #     input_size = X_train.shape[1],\n","    #     hidden_size = (50, Y_train.shape[1]),\n","    #     activation = (torch.nn.ReLU, torch.nn.Sigmoid)\n","    # ).to(device)\n","    # loss_fcn = torch.nn.MSELoss()\n","    # optimizer = torch.optim.Adam(model.parameters())\n","    \n","    # steps = 2\n","    # for k in range(steps):\n","    #     optimizer.zero_grad()  # Clear gradient\n","    #     loss = loss_fcn(model(X_train), Y_train) # Compute train loss\n","    #     loss.backward()  # Backward propagation\n","    #     optimizer.step()  # Learn\n","    # break\n","\n"],"metadata":{"id":"ifb2XGpbaf7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.describe()"],"metadata":{"id":"al_FzlDctJJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Feedforward(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, activation):\n","        super(Feedforward, self).__init__()\n","        activation = listify(activation)\n","        hidden_size = listify(hidden_size)\n","        q = hidden_size[0]\n","        L = [torch.nn.Linear(input_size, q)]\n","        p = q\n","        for f, q in zip(activation, hidden_size):\n","            L.append(f())\n","            L.append(torch.nn.Linear(p, q))\n","            p = q\n","        self.nn = torch.nn.Sequential(*L)\n","\n","    def forward(self, x, w):\n","        self.prop = self.model(x)\n","        self.pred = ((self.prop * w) @ b).squeeze()\n","        return self.pred\n","\n","\n","\n","\n","model = Feedforward(\n","    input_size = x_train.shape[1],\n","    hidden_size = (50, 6),\n","    activation = (torch.nn.ReLU, torch.nn.Sigmoid)\n",").to(device)\n","loss_fcn = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","model.eval()\n","print(f'Test loss before training {loss_fcn(model(x_test, w_test), y_test)}')\n","\n","model.train()\n","epoch = 1000\n","for epoch in range(epoch):\n","    optimizer.zero_grad()             # Clear gradient\n","    y_pred = model(x_train, w_train)  # Forward propagation\n","    loss = loss_fcn(y_pred, y_train)  # Compute Loss\n","    if epoch % 100 == 0:\n","        print(f'Epoch {epoch}: train loss {loss}')\n","    loss.backward()                   # Backward propagation\n","    optimizer.step()                  # Learn\n","\n","model.eval()\n","print(f'Test loss after training {loss_fcn(model(x_test, w_test), y_test)}')"],"metadata":{"id":"eI0ibchBIF-3"},"execution_count":null,"outputs":[]}]}